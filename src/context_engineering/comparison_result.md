# Context Engineering Comparison

## Output
```json
{
  "semantic_kernel": {
    "framework": "Semantic Kernel",
    "resposta": "",
    "contexto_final": {
      "instrucoes": "Você é um especialista em workshops técnicos. Seja objetivo, organize a resposta em tópicos e finalize com um checklist para o time.",
      "ultima_resposta": ""
    },
    "metricas": [
      {
        "framework": "Agent Framework",
        "operacoes": 3,
        "tempo_total_ms": 34192.86,
        "tempo_medio_ms": 11397.62
      },
      {
        "framework": "Semantic Kernel",
        "operacoes": 5,
        "tempo_total_ms": 43958.26,
        "tempo_medio_ms": 8791.65
      }
    ]
  },
  "agent_framework": {
    "framework": "Agent Framework",
    "respostas": [
      "Ótimo — com o briefing em mente (comparar técnicas de engenharia de contexto usando Semantic Kernel — SK — e um Agent Framework), sugiro a seguinte estrutura inicial para abrir o evento. Em seguida listo próximos passos práticos e peço sua confirmação como especialista.\n\nSeções sugeridas para iniciar o evento (com objetivo e duração indicativa)\n1. Abertura e objetivos (10–15 min)\n   - Objetivo do workshop, resultados esperados (p. ex. matriz comparativa, notebooks/demos, recomendações).\n   - Critérios de sucesso.\n2. Agenda e logística (5–10 min)\n   - Cronograma, pausas, canais de comunicação, repositório e onde subir artefatos.\n3. Conhecimento prévio & roles dos participantes (10 min)\n   - Levantamento rápido do nível dos participantes; definição de papéis (facilitador, expert SK, expert Agent Framework, infra, rapporteurs).\n4. Visão geral: engenharia de contexto (15 min)\n   - Conceitos chave: prompt templates, contexto persistente (memória), retrieval-augmented generation (RAG), gerenciamento de janela de contexto, custo/latência, avaliação de qualidade.\n5. Introdução a SK (Semantic Kernel) — teoria e demo curta (20–30 min)\n   - Arquitetura, como SK trata contexto/memória, exemplos de patterns (skills, planners).\n   - Demo ao vivo ou gravada: pipeline mínimo.\n6. Introdução ao Agent Framework (teoria e demo curta) (20–30 min)\n   - Escolher o(s) framework(s) alvo (ex.: LangChain Agents, AutoGen, Microsoft ORK, etc.). Apresentar como o framework trata contexto, ferramentas e orquestração.\n   - Demo ao vivo ou gravada.\n7. Critérios de comparação & métricas (10–15 min)\n   - Métricas propostas: acurácia/relevância, taxa de alucinação, latência, custo por request, facilidade de integração, manutenibilidade, escalabilidade.\n8. Hands-on / Labs (breakout) — aplicar ambas as abordagens ao mesmo problema (60–90 min)\n   - Problema padrão (ex.: assistente de suporte que usa documentos + memória por usuário + ferramentas externas).\n   - Grupos: um usando SK, outro usando Agent Framework; scripts/notebooks base disponibilizados.\n9. Demonstrações dos grupos e discussão (30–45 min)\n   - Cada grupo apresenta resultados, trade-offs e dificuldades.\n10. Análise comparativa & conclusão (20–30 min)\n    - Consolidação da matriz comparativa, lições aprendidas, recomendações.\n11. Próximos passos operacionais e encerramento (10–15 min)\n    - Ações pós-workshop, responsáveis, cronograma de follow-up, feedback.\n\nItens operacionais e técnicos (pré-workshop)\n- Repositório Git com:\n  - Notebooks/exemplos base para SK e para o Agent Framework.\n  - Scripts de instalação e instruções (README).\n- Pré-requisitos para participantes:\n  - Conta(s) de API (LLM), chaves, Python/Node/.NET conforme necessário.\n  - Instalação de dependências (script pip/conda; instruções para Semantic Kernel se for .NET/Python).\n- Dados de exemplo: pequenos corpus de documentos, prompts de avaliação, casos de teste.\n- Infra/ops:\n  - Ambiente de backup (demos gravadas) caso a live falhe.\n  - Canal para suporte técnico durante os labs (Slack/Teams).\n- Papel dos facilitadores:\n  - Host/moderador, expert SK, expert Agent Framework, facilitador de breakouts, responsável por infra, nota-taker.\n\nSugestão de critérios de avaliação prática (para uso nos labs)\n- Relevância/resposta correta (qualidade).\n- Taxa de alucinação.\n- Latência média por interação.\n- Tokens/custo por execução.\n- Facilidade de integração / boilerplate.\n- Flexibilidade para estender com ferramentas externas.\n- Robustez frente a contexto grande (ex.: truncamento/retentiva).\n\nPróximos passos concretos (checklist com responsáveis e prazos)\n1. Escolher e confirmar qual Agent Framework vamos comparar (responsável: você/especialista) — prazo: 2 dias.\n2. Preparar repositório com demos mínimos para SK e Agent Framework (responsável: dev/infra) — prazo: 5 dias.\n3. Definir dataset de exemplo e casos de teste para avaliação (responsável: SME de domínio) — prazo: 5 dias.\n4. Preparar instruções de pré-work para participantes (instalação, chaves) — prazo: 3 dias.\n5. Designar facilitadores e roles no workshop (responsável: você) — prazo: 2 dias.\n6. Criar rubrica de avaliação e planilha para coletar métricas durante os labs (responsável: facilitador de avaliação) — prazo: 4 dias.\n7. Teste técnico (dry-run) das demos ao vivo e fallback gravado (responsável: infra) — prazo: 2 dias antes do evento.\n\nVerificação com o especialista\n- Você, como especialista em SK/Agent Framework, concorda com:\n  1) as seções propostas e suas durações?\n  2) os critérios de comparação sugeridos?\n  3) a divisão de responsabilidades e cronograma de próximos passos?\nSe concordar, indico que confirmemos:\n- qual Agent Framework específico usar (indique o nome);\n- se preferimos demos ao vivo ou gravadas como fallback;\n- quem será o responsável por cada item da checklist (posso consolidar numa planilha).\n\nQuer que eu já envie um e-mail modelo ou uma task list em formato de planilha para os responsáveis? Concorda com as seções e próximos passos acima?",
      "Perfeito — abaixo uma proposta prática concreta para incorporar uma seção do workshop focada em “alterações de contexto” (incluindo quando remover contexto para evitar respostas irrelevantes). No final pergunto pontos para você confirmar como especialista.\n\nResumo da seção prática (objetivo e duração)\n- Objetivo: experimentar estratégias de inclusão/remoção/transformação de contexto e medir impacto em relevância, alucinação e custo.\n- Duração sugerida: 60–90 minutos (divididos em setup 10–15 min, execuções/experimentos 35–55 min, apresentação rápida 10–20 min).\n\nLearning outcomes esperados\n- Saber quando remover ou reduzir contexto melhora a resposta.\n- Saber aplicar filtros (temporal, semântico, metadata), sumarização e “context reset”.\n- Medir trade-offs: perda de contexto vs redução de irrelevância/custo.\n\nSetup prévio (o que preparar)\n- Repositório com 1 notebook/script por abordagem (SK e Agent Framework) contendo toggles para as variações de contexto.\n- Dataset curto e realista (ex.: KB de suporte + histórico de conversas por usuário + registros com timestamps) — 20–30 casos de teste.\n- Ambiente com chaves e dependências instaladas; vector DB (ex.: Pinecone/Redis/Weaviate) ou mock retriever.\n- Planilha de coleta de métricas e rubrica de avaliação (relevância 1–5, presença de alucinação 0/1, latência, tokens/custo, anotação livre).\n\nExperimentos práticos sugeridos (cada grupo executa 4–6 variações)\n1. Baseline (tudo): enviar todo o histórico/contexto disponível.\n   - Métrica esperada: melhor continuidade; risco de irrelevância/alucinação por contexto desatualizado.\n2. Remoção total (clear/reset memory): limpar memória/contexto antes da requisição.\n   - Use quando histórico confunde o objetivo atual; avalie perda de contexto.\n3. Filtro por metadata/temporal (janela temporal): só usar entradas antes/apos certa data ou só últimos N turnos.\n   - Bom para evitar usar info desatualizada.\n4. Recuperação restrita (retriever top-k baixo / limiar de similaridade): reduzir k ou aplicar threshold de similaridade/score.\n   - Enfoca documentos mais relevantes, reduz tokens.\n5. Sumarização incremental (compress prior turns): substituir múltiplos turns por um resumo curto (auto-summarize).\n   - Reduz tokens e preserva facts essenciais.\n6. Pinned facts / hot-list: manter apenas facts “fixos” (p.ex. perfil do usuário) e não mais o restante do histórico.\n7. Explicit “ignore previous context” prompt/system message: adicionar instrução forte para ignorar histórico anterior ao processar.\n   - Testar como o modelo respeita instruções versus removendo de fato.\n8. Context weighting / toolbox: marcar partes do contexto como “relevantes”/“irrelevantes” e priorizar via prompt engineering.\n9. Chunking + RAG por tópico: dividir docs em tópicos e recuperar apenas o tópico relacionado à pergunta.\n10. Stress test (contexto longo): forçar truncamento e avaliar comportamentos de fallback.\n\nComo implementar rápido (SK vs Agent Framework)\n- Semantic Kernel (SK)\n  - Ajustar semantic memory retriever: parâmetro top_k / filtro de metadados.\n  - Criar skill de “context-sanitizer” que faz: (a) prune por metadata, (b) sumariza entradas antigas, (c) remove registros.\n  - Para “remoção total”: chamar API de remoção/esvaziamento do store (ou passar prompt sem inserir memória).\n- Agent Framework (ex.: LangChain / AutoGen / ORK)\n  - Alterar objeto de memória (ConversationBuffer / SummaryMemory) — usar memory.clear() ou remover por metadata.\n  - Ajustar retriever.search_kwargs (k, score_threshold) ou usar filtros do vectorstore.\n  - Interceptar o envio de histórico ao LLM (middleware) para controlar quais mensagens vão no prompt.\n\nFluxo prático durante o lab (passo a passo)\n1. Setup rápido (10–15 min): carregar notebook; confirmar chaves; executar caso de teste de sanity.\n2. Execução dos experimentos (35–55 min):\n   - Rodar Baseline e coletar métricas/response.\n   - Rodar 3–4 variações selecionadas (cada variação: executar 5–10 queries padronizadas).\n   - Registrar resultados no template (planilha).\n3. Análise rápida (10–15 min): comparar métricas, anotar exemplos qualitativos onde remoção ajudou/ferrou.\n4. Apresentação breve (5–10 min por grupo): insights e recomendações.\n\nMétricas e rubrica (rápido)\n- Relevância/hit rate (1–5).\n- Alucinação (0/1) + descrição do item alucinado.\n- Latência média (ms).\n- Tokens consumidos / custo estimado.\n- Impacto na continuidade (1–5).\n- Observações qualitativas (quando remoção foi benéfica e quando não).\n\nInstrumentação / logs a coletar\n- Conversa original + contexto usado por experimento.\n- Conteúdo efetivo do prompt enviado (para auditoria).\n- Resposta do LLM e metadados (scores, latency).\n- Versão do retriever (k, thresholds) e snapshot do memory store (IDs dos registros usados).\n\nPapel dos facilitadores\n- Facilitador SK: ajuda com o skill de sanitização e acesso à memória.\n- Facilitador Agent Framework: ajuda a limpar/filtrar memória e ajustar retriever.\n- Avaliador: aplica rubrica e consolida planilha.\n- Infra: resolve chaves, vector DB e problemas de execução.\n\nEntregáveis ao final do lab\n- Planilha com comparativos e exemplos.\n- Notebook com toggles para replicar as variações.\n- Slides/resumo com recomendações (quando remover contexto e quais estratégias usar).\n\nRiscos e mitigação\n- Demo ao vivo pode falhar (fallback: notebooks com resultados pré-gerados e gravação).\n- Diferença de skill dos participantes (ter scripts “run-all” para não exigir conhecimento profundo).\n\nPerguntas para você (verificação do especialista)\n1. Qual Agent Framework específico vamos usar (ex.: LangChain, AutoGen, ORK)? Preciso disso para preparar notebooks específicos.\n2. Preferimos demos ao vivo ou demos gravadas como fallback?\n3. Quer que eu gere os notebooks com toggles para estes experimentos e a planilha de métricas? Posso preparar em 3–5 dias.\n4. Concorda com duração proposta (60–90 min) e com a lista de variações experimentais? Quer incluir mais alguma estratégia (p.ex. verificação automática de factualidade)?\n\nSe concordar, eu preparo:\n- 1 notebook SK + 1 notebook Agent Framework com 6 variações já implementadas (baseline, clear, top-k, summary, temporal window, pinned facts).\n- Planilha de coleta de métricas e uma página de instruções para os grupos.\n\nQuer que eu já comece a montar esses materiais?",
      "Ótimo — abaixo a checklist final pronta para encerrar o workshop. Inclui tarefas a executar durante o fechamento, ações pós-workshop com prazos sugeridos, entregáveis e critérios de aceite. Peço que confirme proprietários (nomes) e duas decisões pendentes (qual Agent Framework e demo ao vivo vs gravada).\n\nChecklist final para encerrar o workshop\n\n1) Encerramento imediato (durante os últimos 10–15 min)\n- [ ] Coletar e registrar consenso rápido: 3 principais insights / trade-offs (cada grupo/repórter).\n- [ ] Finalizar e salvar a matriz comparativa provisória (SK vs Agent Framework) com notas qualitativas.\n- [ ] Identificar decisões pendentes e bloqueios (lista curta).\n- [ ] Atribuir responsáveis e prazos iniciais para cada pendência (ver seção “Decisões pendentes”).\n- [ ] Aplicar survey rápido de satisfação/feedback (link enviado a todos).\n- [ ] Confirmar que gravações, slides e notebooks estão sendo coletados/arquivados.\n\n2) Ações imediatas pós-workshop (0–3 dias)\n- [ ] Consolidar planilha de métricas (avaliador) — agregar relevância, alucinação, latência, tokens/custo.\n  - Prazo sugerido: 2 dias\n- [ ] Reunir e commitar notebooks finais (SK + Agent Framework) no repositório; incluir run-all scripts e README (dev/infra).\n  - Prazo sugerido: 3 dias\n- [ ] Subir gravações, slides e artefatos para o repositório ou drive compartilhado e enviar e-mail de follow-up com links (facilitador/project owner).\n  - Prazo sugerido: 2 dias\n- [ ] Limpar credenciais usadas nos demos (rotacionar chaves / remover arquivos locais) e registrar ações de segurança (infra).\n  - Prazo sugerido: 1 dia\n\n3) Entregáveis a médio prazo (3–10 dias)\n- [ ] Relatório executivo + recomendações (SME + facilitador) — incluir resumo da matriz, decisões recomendadas e riscos.\n  - Prazo sugerido: 7 dias\n- [ ] Notebooks definitivos com 6 variações implementadas (baseline, clear, top-k, summary, temporal window, pinned facts) e exemplos de saída (dev).\n  - Prazo sugerido: 5 dias\n- [ ] Planilha final com rubrica preenchida e exemplos anotados (avaliador).\n  - Prazo sugerido: 5 dias\n- [ ] Lista de “quando remover contexto” com critérios acionáveis (SME/facilitador).\n  - Prazo sugerido: 5 dias\n- [ ] Sessão de follow-up / decisão (30–60 min) para:\n  - validar recomendações,\n  - confirmar Agent Framework final,\n  - autorizar próximos passos (piloto/prova de conceito).\n  - Prazo sugerido: agendar dentro de 10 dias\n\n4) Deliverables e critérios de aceite (Definition of Done)\n- Repositório com:\n  - Notebooks SK + Agent Framework que rodam com um comando (run-all) e README reproduzível.\n  - Scripts de instalação e dependências.\n- Planilha consolidada com métricas e evidências (pelo menos 5 queries por variação).\n- Relatório executivo com recomendações claras e listagem de riscos/mitigações.\n- Gravações + slides disponíveis a todos.\n- Critérios de aceite: notebooks reproduzíveis por um engenheiro externo seguindo o README; métricas consolidada e validação qualitativa por SME; sign-off do project owner.\n\n5) Governança, segurança e compliance\n- [ ] Verificar e remover PII sensível dos datasets (SME/Compliance).\n- [ ] Garantir que chaves usadas em demos sejam rotacionadas e não estejam no repo (infra).\n- [ ] Arquivar snapshot do vector DB (se necessário) e documentar retention policy.\n\n6) Riscos remanescentes e mitigação\n- Risco: diferenças de skill entre grupos → mitigação: run-all & notebook “quick-run”.\n- Risco: falha de demo ao vivo → mitigação: publicar gravação e exemplos pré-gerados.\n- Risco: decisão sobre qual Agent Framework → mitigação: decidir em 2 dias ou executar mini-PoC.\n\n7) Decisões pendentes (precisam de resposta rápida)\n- Decidir qual Agent Framework usar para os notebooks oficiais (sugestão: indicar nome) — prazo: 2 dias.\n- Decidir se demos finais serão ao vivo ou gravadas como fallback — prazo: 2 dias.\n- Indicar responsável pela consolidação final das métricas / relatório (nome).\n- Indicar owner do repositório e PR reviewer (nome).\n\n8) Comunicação e follow-up\n- [ ] Enviar e-mail de encerramento com links para repositório, gravação, planilha e tarefas atribuídas (facilitador).\n  - Prazo sugerido: 1 dia\n- [ ] Agendar retrospectiva / decisão final (agenda: revisar relatório + aprovar próximo passo) — convidar stakeholders.\n\n9) Proposta de prazos-resumo (sugestão)\n- 2 dias: escolher Agent Framework e tipo de demo; atribuir owners.\n- 3 dias: reunir gravações, slides e run-all notebooks mínimos.\n- 5 dias: notebooks finais + planilha consolidada.\n- 7 dias: relatório executivo.\n- 10 dias: reunião de decisão/follow-up.\n\nDeseja que eu:\n- gere a planilha template com as métricas já formatadas?\n- monte o e-mail modelo de follow-up com a lista de tarefas e links?\n- crie a checklist no Google Sheets e atribua tarefas (você indica os emails dos owners)?\n\nConfirmação necessária (responda rápido)\n1) Você concorda com a checklist e prazos sugeridos?\n2) Qual Agent Framework escolhemos (nome)?\n3) Preferimos demo ao vivo ou fallback gravado?\n4) Quem assume (nomes) esses itens críticos: consolidação de métricas, owner do repo, responsável pelo relatório?\n\nQuando confirmar, eu preparo os artefatos (planilha + e-mail modelo) e atribuo as tasks."
    ],
    "contexto_final": {
      "briefing": "Reforçar próximos passos e validar checklist final.",
      "ultimo_sumario": "Perfeito — abaixo uma proposta prática concreta para incorporar uma seção do workshop focada em “alterações de contexto” (incluindo quando remover contexto para evitar respostas irrelevantes). No final pergunto pontos para você confirmar como especialista.\n\nResumo da seção prática (objetivo e duração)\n- Objetivo: experimentar estratégias de inclusão/remoção/transformação de contexto e medir impacto em relevância, alucinação e custo.\n- Duração sugerida: 60–90 minutos (divididos em setup 10–15 min, execuções/experimentos 35–55 min, apresentação rápida 10–20 min).\n\nLearning outcomes esperados\n- Saber quando remover ou reduzir contexto melhora a resposta.\n- Saber aplicar filtros (temporal, semântico, metadata), sumarização e “context reset”.\n- Medir trade-offs: perda de contexto vs redução de irrelevância/custo.\n\nSetup prévio (o que preparar)\n- Repositório com 1 notebook/script por abordagem (SK e Agent Framework) contendo toggles para as variações de contexto.\n- Dataset curto e realista (ex.: KB de suporte + histórico de conversas por usuário + registros com timestamps) — 20–30 casos de teste.\n- Ambiente com chaves e dependências instaladas; vector DB (ex.: Pinecone/Redis/Weaviate) ou mock retriever.\n- Planilha de coleta de métricas e rubrica de avaliação (relevância 1–5, presença de alucinação 0/1, latência, tokens/custo, anotação livre).\n\nExperimentos práticos sugeridos (cada grupo executa 4–6 variações)\n1. Baseline (tudo): enviar todo o histórico/contexto disponível.\n   - Métrica esperada: melhor continuidade; risco de irrelevância/alucinação por contexto desatualizado.\n2. Remoção total (clear/reset memory): limpar memória/contexto antes da requisição.\n   - Use quando histórico confunde o objetivo atual; avalie perda de contexto.\n3. Filtro por metadata/temporal (janela temporal): só usar entradas antes/apos certa data ou só últimos N turnos.\n   - Bom para evitar usar info desatualizada.\n4. Recuperação restrita (retriever top-k baixo / limiar de similaridade): reduzir k ou aplicar threshold de similaridade/score.\n   - Enfoca documentos mais relevantes, reduz tokens.\n5. Sumarização incremental (compress prior turns): substituir múltiplos turns por um resumo curto (auto-summarize).\n   - Reduz tokens e preserva facts essenciais.\n6. Pinned facts / hot-list: manter apenas facts “fixos” (p.ex. perfil do usuário) e não mais o restante do histórico.\n7. Explicit “ignore previous context” prompt/system message: adicionar instrução forte para ignorar histórico anterior ao processar.\n   - Testar como o modelo respeita instruções versus removendo de fato.\n8. Context weighting / toolbox: marcar partes do contexto como “relevantes”/“irrelevantes” e priorizar via prompt engineering.\n9. Chunking + RAG por tópico: dividir docs em tópicos e recuperar apenas o tópico relacionado à pergunta.\n10. Stress test (contexto longo): forçar truncamento e avaliar comportamentos de fallback.\n\nComo implementar rápido (SK vs Agent Framework)\n- Semantic Kernel (SK)\n  - Ajustar semantic memory retriever: parâmetro top_k / filtro de metadados.\n  - Criar skill de “context-sanitizer” que faz: (a) prune por metadata, (b) sumariza entradas antigas, (c) remove registros.\n  - Para “remoção total”: chamar API de remoção/esvaziamento do store (ou passar prompt sem inserir memória).\n- Agent Framework (ex.: LangChain / AutoGen / ORK)\n  - Alterar objeto de memória (ConversationBuffer / SummaryMemory) — usar memory.clear() ou remover por metadata.\n  - Ajustar retriever.search_kwargs (k, score_threshold) ou usar filtros do vectorstore.\n  - Interceptar o envio de histórico ao LLM (middleware) para controlar quais mensagens vão no prompt.\n\nFluxo prático durante o lab (passo a passo)\n1. Setup rápido (10–15 min): carregar notebook; confirmar chaves; executar caso de teste de sanity.\n2. Execução dos experimentos (35–55 min):\n   - Rodar Baseline e coletar métricas/response.\n   - Rodar 3–4 variações selecionadas (cada variação: executar 5–10 queries padronizadas).\n   - Registrar resultados no template (planilha).\n3. Análise rápida (10–15 min): comparar métricas, anotar exemplos qualitativos onde remoção ajudou/ferrou.\n4. Apresentação breve (5–10 min por grupo): insights e recomendações.\n\nMétricas e rubrica (rápido)\n- Relevância/hit rate (1–5).\n- Alucinação (0/1) + descrição do item alucinado.\n- Latência média (ms).\n- Tokens consumidos / custo estimado.\n- Impacto na continuidade (1–5).\n- Observações qualitativas (quando remoção foi benéfica e quando não).\n\nInstrumentação / logs a coletar\n- Conversa original + contexto usado por experimento.\n- Conteúdo efetivo do prompt enviado (para auditoria).\n- Resposta do LLM e metadados (scores, latency).\n- Versão do retriever (k, thresholds) e snapshot do memory store (IDs dos registros usados).\n\nPapel dos facilitadores\n- Facilitador SK: ajuda com o skill de sanitização e acesso à memória.\n- Facilitador Agent Framework: ajuda a limpar/filtrar memória e ajustar retriever.\n- Avaliador: aplica rubrica e consolida planilha.\n- Infra: resolve chaves, vector DB e problemas de execução.\n\nEntregáveis ao final do lab\n- Planilha com comparativos e exemplos.\n- Notebook com toggles para replicar as variações.\n- Slides/resumo com recomendações (quando remover contexto e quais estratégias usar).\n\nRiscos e mitigação\n- Demo ao vivo pode falhar (fallback: notebooks com resultados pré-gerados e gravação).\n- Diferença de skill dos participantes (ter scripts “run-all” para não exigir conhecimento profundo).\n\nPerguntas para você (verificação do especialista)\n1. Qual Agent Framework específico vamos usar (ex.: LangChain, AutoGen, ORK)? Preciso disso para preparar notebooks específicos.\n2. Preferimos demos ao vivo ou demos gravadas como fallback?\n3. Quer que eu gere os notebooks com toggles para estes experimentos e a planilha de métricas? Posso preparar em 3–5 dias.\n4. Concorda com duração proposta (60–90 min) e com a lista de variações experimentais? Quer incluir mais alguma estratégia (p.ex. verificação automática de factualidade)?\n\nSe concordar, eu preparo:\n- 1 notebook SK + 1 notebook Agent Framework com 6 variações já implementadas (baseline, clear, top-k, summary, temporal window, pinned facts).\n- Planilha de coleta de métricas e uma página de instruções para os grupos.\n\nQuer que eu já comece a montar esses materiais?",
      "ultima_resposta": "Ótimo — abaixo a checklist final pronta para encerrar o workshop. Inclui tarefas a executar durante o fechamento, ações pós-workshop com prazos sugeridos, entregáveis e critérios de aceite. Peço que confirme proprietários (nomes) e duas decisões pendentes (qual Agent Framework e demo ao vivo vs gravada).\n\nChecklist final para encerrar o workshop\n\n1) Encerramento imediato (durante os últimos 10–15 min)\n- [ ] Coletar e registrar consenso rápido: 3 principais insights / trade-offs (cada grupo/repórter).\n- [ ] Finalizar e salvar a matriz comparativa provisória (SK vs Agent Framework) com notas qualitativas.\n- [ ] Identificar decisões pendentes e bloqueios (lista curta).\n- [ ] Atribuir responsáveis e prazos iniciais para cada pendência (ver seção “Decisões pendentes”).\n- [ ] Aplicar survey rápido de satisfação/feedback (link enviado a todos).\n- [ ] Confirmar que gravações, slides e notebooks estão sendo coletados/arquivados.\n\n2) Ações imediatas pós-workshop (0–3 dias)\n- [ ] Consolidar planilha de métricas (avaliador) — agregar relevância, alucinação, latência, tokens/custo.\n  - Prazo sugerido: 2 dias\n- [ ] Reunir e commitar notebooks finais (SK + Agent Framework) no repositório; incluir run-all scripts e README (dev/infra).\n  - Prazo sugerido: 3 dias\n- [ ] Subir gravações, slides e artefatos para o repositório ou drive compartilhado e enviar e-mail de follow-up com links (facilitador/project owner).\n  - Prazo sugerido: 2 dias\n- [ ] Limpar credenciais usadas nos demos (rotacionar chaves / remover arquivos locais) e registrar ações de segurança (infra).\n  - Prazo sugerido: 1 dia\n\n3) Entregáveis a médio prazo (3–10 dias)\n- [ ] Relatório executivo + recomendações (SME + facilitador) — incluir resumo da matriz, decisões recomendadas e riscos.\n  - Prazo sugerido: 7 dias\n- [ ] Notebooks definitivos com 6 variações implementadas (baseline, clear, top-k, summary, temporal window, pinned facts) e exemplos de saída (dev).\n  - Prazo sugerido: 5 dias\n- [ ] Planilha final com rubrica preenchida e exemplos anotados (avaliador).\n  - Prazo sugerido: 5 dias\n- [ ] Lista de “quando remover contexto” com critérios acionáveis (SME/facilitador).\n  - Prazo sugerido: 5 dias\n- [ ] Sessão de follow-up / decisão (30–60 min) para:\n  - validar recomendações,\n  - confirmar Agent Framework final,\n  - autorizar próximos passos (piloto/prova de conceito).\n  - Prazo sugerido: agendar dentro de 10 dias\n\n4) Deliverables e critérios de aceite (Definition of Done)\n- Repositório com:\n  - Notebooks SK + Agent Framework que rodam com um comando (run-all) e README reproduzível.\n  - Scripts de instalação e dependências.\n- Planilha consolidada com métricas e evidências (pelo menos 5 queries por variação).\n- Relatório executivo com recomendações claras e listagem de riscos/mitigações.\n- Gravações + slides disponíveis a todos.\n- Critérios de aceite: notebooks reproduzíveis por um engenheiro externo seguindo o README; métricas consolidada e validação qualitativa por SME; sign-off do project owner.\n\n5) Governança, segurança e compliance\n- [ ] Verificar e remover PII sensível dos datasets (SME/Compliance).\n- [ ] Garantir que chaves usadas em demos sejam rotacionadas e não estejam no repo (infra).\n- [ ] Arquivar snapshot do vector DB (se necessário) e documentar retention policy.\n\n6) Riscos remanescentes e mitigação\n- Risco: diferenças de skill entre grupos → mitigação: run-all & notebook “quick-run”.\n- Risco: falha de demo ao vivo → mitigação: publicar gravação e exemplos pré-gerados.\n- Risco: decisão sobre qual Agent Framework → mitigação: decidir em 2 dias ou executar mini-PoC.\n\n7) Decisões pendentes (precisam de resposta rápida)\n- Decidir qual Agent Framework usar para os notebooks oficiais (sugestão: indicar nome) — prazo: 2 dias.\n- Decidir se demos finais serão ao vivo ou gravadas como fallback — prazo: 2 dias.\n- Indicar responsável pela consolidação final das métricas / relatório (nome).\n- Indicar owner do repositório e PR reviewer (nome).\n\n8) Comunicação e follow-up\n- [ ] Enviar e-mail de encerramento com links para repositório, gravação, planilha e tarefas atribuídas (facilitador).\n  - Prazo sugerido: 1 dia\n- [ ] Agendar retrospectiva / decisão final (agenda: revisar relatório + aprovar próximo passo) — convidar stakeholders.\n\n9) Proposta de prazos-resumo (sugestão)\n- 2 dias: escolher Agent Framework e tipo de demo; atribuir owners.\n- 3 dias: reunir gravações, slides e run-all notebooks mínimos.\n- 5 dias: notebooks finais + planilha consolidada.\n- 7 dias: relatório executivo.\n- 10 dias: reunião de decisão/follow-up.\n\nDeseja que eu:\n- gere a planilha template com as métricas já formatadas?\n- monte o e-mail modelo de follow-up com a lista de tarefas e links?\n- crie a checklist no Google Sheets e atribua tarefas (você indica os emails dos owners)?\n\nConfirmação necessária (responda rápido)\n1) Você concorda com a checklist e prazos sugeridos?\n2) Qual Agent Framework escolhemos (nome)?\n3) Preferimos demo ao vivo ou fallback gravado?\n4) Quem assume (nomes) esses itens críticos: consolidação de métricas, owner do repo, responsável pelo relatório?\n\nQuando confirmar, eu preparo os artefatos (planilha + e-mail modelo) e atribuo as tasks."
    },
    "metricas": [
      {
        "framework": "Agent Framework",
        "operacoes": 7,
        "tempo_total_ms": 106230.43,
        "tempo_medio_ms": 15175.78
      },
      {
        "framework": "Semantic Kernel",
        "operacoes": 5,
        "tempo_total_ms": 43958.26,
        "tempo_medio_ms": 8791.65
      }
    ]
  },
  "comparativo": [
    {
      "framework": "Agent Framework",
      "operacoes": 7,
      "tempo_total_ms": 106230.43,
      "tempo_medio_ms": 15175.78
    },
    {
      "framework": "Semantic Kernel",
      "operacoes": 5,
      "tempo_total_ms": 43958.26,
      "tempo_medio_ms": 8791.65
    }
  ]
}
```

## Logs
```text
2025-10-24 12:52:37,187 [WARNING] When conversation_id is set, store must be True for service-managed threads. Automatically setting store=True.
2025-10-24 12:53:16,664 [WARNING] When conversation_id is set, store must be True for service-managed threads. Automatically setting store=True.
```
